{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "political-lyric",
   "metadata": {},
   "source": [
    "# NLP Task 2-b\n",
    "In this step you will implement a “vanilla” model of the architecture. \n",
    "For this you will need to use Py- Torch or Tensorflow/Keras functional API and various layers\n",
    "like Input, Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, concatenate, Dense, etc. as well as\n",
    "other utility functions such as Tokenizer. For some of the parameters, you should consider the values\n",
    "suggested in Table 1. Please note that those values are “typical” but not necessarily optimal. First, you need to tokenize the texts and cut/pad them to a common max length size. Then you derive the train and test samples and labels. The “vanilla” model should contain the Embedding layer, a single convolution layer of only one block followed by a max-pooling layer and a single dense layer. You will report the classification accuracy of this model. (18 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "electrical-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random, sys\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Conv1D, MaxPooling1D, Flatten, Concatenate\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pydot\n",
    "import graphviz\n",
    "# Collect Text \n",
    "import pandas as pd\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-spectacular",
   "metadata": {},
   "source": [
    "#### Parameters (Recommendation)\n",
    "\n",
    "W or number of convolution blocks in each layer: 2-5.  \n",
    "L or number of consecuetive convolution-pooling layers: 2-4  \n",
    "Maximal length of each review sequence: 300-500    \n",
    "Dimension of word embbeddings: 150-300.  \n",
    "Train : Test split of the data samples: 4:1 or 9:1.  \n",
    "Number of filters in each convolution layer: 10-50.  \n",
    "Kernel size in each convolution block: 1-5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-collapse",
   "metadata": {},
   "source": [
    "Here we set up the parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "legislative-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of convolution blocks in each layer\n",
    "# W = 1\n",
    "# Number of consecuetive convolution-pooling layers\n",
    "# L = 1\n",
    "#  Max lenght of each review sequence\n",
    "UNIFORM_LENGTH = 600\n",
    "# The dimension of the word embeddings\n",
    "WORD_EMBEDDING_DIM = 150\n",
    "# Number of filters in each convolution layer\n",
    "CONV_FILTERS = 10\n",
    "# Kernel size in each convolution block\n",
    "KERNEL_SIZE = 1\n",
    "# Vocabulary: number of most frequent words\n",
    "VOCABULARY = 30000\n",
    "# POOL_SIZE: Downsamples the input representation by taking the maximum value \n",
    "POOL_SIZE = 2\n",
    "# Training and evaluation:\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-final",
   "metadata": {},
   "source": [
    "### Loading and preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-taylor",
   "metadata": {},
   "source": [
    "Loading the pre-processed data and splitting them into train and test sets. Random state is fixed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "operational-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training examples: 40000\n",
      "No. of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('review_preprocessed.csv')\n",
    "training_data = df.sample(frac=0.8, random_state=25)\n",
    "testing_data = df.drop(training_data.index)\n",
    "\n",
    "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
    "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "educated-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting the pandas dataframe to lists and numpy arrays\n",
    "X_train = training_data.loc[:,'review'].to_list()\n",
    "X_test = testing_data.loc[:,'review'].to_list()\n",
    "y_train = training_data.loc[:,'polarity'].to_numpy()\n",
    "y_test = testing_data.loc[:,'polarity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-albany",
   "metadata": {},
   "source": [
    "The Tokenize class helps us to vectorize a text corpus by tunring them into a sequence of integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "pharmaceutical-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = Tokenizer(num_words = VOCABULARY)\n",
    "\n",
    "t.fit_on_texts(X_train)\n",
    "X_train_enc = t.texts_to_sequences(X_train)\n",
    "X_test_enc = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-kitty",
   "metadata": {},
   "source": [
    "Here we force a uniform length for each review. Longer reviews are truncated and shorted reviews and padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "choice-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_enc, maxlen=UNIFORM_LENGTH)\n",
    "X_test_pad = pad_sequences(X_test_enc, maxlen=UNIFORM_LENGTH)\n",
    "\n",
    "X_train = X_train_pad\n",
    "X_test = X_test_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-doctrine",
   "metadata": {},
   "source": [
    "## CNN vanilla model with keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "regional-literacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1fc0d695820>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_existing_code(VOCABULARY, WORD_EMBEDDING_DIM, UNIFORM_LENGTH, CONV_FILTERS, KERNEL_SIZE):\n",
    "    inputs = Input(shape=(UNIFORM_LENGTH,))\n",
    "    x = Embedding(VOCABULARY, WORD_EMBEDDING_DIM, input_length=UNIFORM_LENGTH)(inputs)\n",
    "    x = Conv1D(filters=CONV_FILTERS, kernel_size=KERNEL_SIZE, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=POOL_SIZE)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense (100, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.Adam(learning_rate=lr), ### Uncomment this to tune learning rate\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    VOCABULARY = 30000\n",
    "    WORD_EMBEDDING_DIM = hp.Int(\"output_dim\", min_value=150, max_value=300, step=50)\n",
    "    UNIFORM_LENGTH = 600\n",
    "    CONV_FILTERS = hp.Int(\"filters\", min_value=10, max_value=50, step=10)\n",
    "    KERNEL_SIZE = hp.Int(\"kernel_size\", min_value=1, max_value=5, step=1)\n",
    "\n",
    "    # call existing model-building code with the hyperparameter values.\n",
    "    model = call_existing_code(\n",
    "        VOCABULARY=VOCABULARY, \n",
    "        WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, \n",
    "        UNIFORM_LENGTH=UNIFORM_LENGTH, \n",
    "        CONV_FILTERS=CONV_FILTERS, \n",
    "        KERNEL_SIZE=KERNEL_SIZE\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "build_model(kt.HyperParameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "numerical-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 3\n",
      "output_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 150, 'max_value': 300, 'step': 50, 'sampling': None}\n",
      "filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 50, 'step': 10, 'sampling': None}\n",
      "kernel_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 5, 'step': 1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    overwrite=True)\n",
    "    \n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "loaded-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 02m 28s]\n",
      "val_accuracy: 0.9053000211715698\n",
      "\n",
      "Best val_accuracy So Far: 0.9053000211715698\n",
      "Total elapsed time: 00h 10m 31s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=2, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "strong-lexington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameter for CONV_FILTERS:  30\n",
      "Optimal parameter for WORD_EMBEDDING_DIM:  200\n",
      "Optimal parameter for KERNEL_SIZE:  4\n"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "print(\"Optimal parameter for CONV_FILTERS: \", best_hps.get('filters'))\n",
    "print(\"Optimal parameter for WORD_EMBEDDING_DIM: \", best_hps.get('output_dim'))\n",
    "print(\"Optimal parameter for KERNEL_SIZE: \", best_hps.get('kernel_size'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-government",
   "metadata": {},
   "source": [
    "## Conclusion of Task 2(b)\n",
    "The optimal parameters obtained in task 2(b) were a word embedding dimension of 200 and 30 convolution filters. Those are now used in task 2(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-combining",
   "metadata": {},
   "source": [
    "# Task 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bound-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_elaborate_model(vocabulary, word_embedding_dim, uniform_length, conv_filters, W, L):\n",
    "    inputs = Input(shape=(uniform_length,))\n",
    "    x = Embedding(vocabulary, word_embedding_dim, input_length=uniform_length)(inputs)\n",
    "    \n",
    "    #convolutional layers\n",
    "    for l in range (L):\n",
    "        c = []\n",
    "        for w in range(W):\n",
    "            if l == 0:\n",
    "                c.append (Conv1D(filters=conv_filters, kernel_size=w+1, activation='relu')(x)) #ONLY x in first run!!!\n",
    "            else:\n",
    "                c.append (Conv1D(filters=conv_filters, kernel_size=w+1, activation='relu')(p[w]))\n",
    "        p = []\n",
    "        for w in range(W):    \n",
    "            p.append (MaxPooling1D(pool_size=POOL_SIZE)(c[w]))\n",
    "\n",
    "    x = Concatenate(axis=1)(p) #axis correct???\n",
    "    x = Flatten()(x)\n",
    "    x = Dense (100, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    #plot_model(model, to_file='elaborate_model.png', show_shapes=True)\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.Adam(learning_rate=lr), ### Uncomment this to tune learning rate\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "elaborate_model = set_elaborate_model (VOCABULARY, 200, UNIFORM_LENGTH, 30, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "initial-electronics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 120s 191ms/step - loss: 0.3209 - accuracy: 0.8513 - val_loss: 0.2370 - val_accuracy: 0.9048\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 117s 188ms/step - loss: 0.1310 - accuracy: 0.9529 - val_loss: 0.2646 - val_accuracy: 0.8991\n"
     ]
    }
   ],
   "source": [
    "history = elaborate_model.fit (X_train, y_train, batch_size=BATCH_SIZE, epochs=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "meaning-diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8991000056266785\n"
     ]
    }
   ],
   "source": [
    "print (history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "appropriate-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 86s 137ms/step - loss: 0.3363 - accuracy: 0.8432 - val_loss: 0.2653 - val_accuracy: 0.8912\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 88s 141ms/step - loss: 0.1428 - accuracy: 0.9472 - val_loss: 0.2660 - val_accuracy: 0.8958\n",
      "Validation Accuracy with W=2 and L=2: 0.895799994468689.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 92s 147ms/step - loss: 0.3350 - accuracy: 0.8388 - val_loss: 0.2726 - val_accuracy: 0.8883\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 98s 156ms/step - loss: 0.1349 - accuracy: 0.9518 - val_loss: 0.3080 - val_accuracy: 0.8861\n",
      "Validation Accuracy with W=2 and L=3: 0.8860999941825867.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 105s 167ms/step - loss: 0.3805 - accuracy: 0.8059 - val_loss: 0.2637 - val_accuracy: 0.8936\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 112s 179ms/step - loss: 0.1626 - accuracy: 0.9398 - val_loss: 0.2659 - val_accuracy: 0.8940\n",
      "Validation Accuracy with W=2 and L=4: 0.8939999938011169.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 137s 219ms/step - loss: 0.3216 - accuracy: 0.8524 - val_loss: 0.2564 - val_accuracy: 0.8943\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 118s 189ms/step - loss: 0.1282 - accuracy: 0.9536 - val_loss: 0.2720 - val_accuracy: 0.8982\n",
      "Validation Accuracy with W=3 and L=2: 0.8981999754905701.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 125s 198ms/step - loss: 0.3426 - accuracy: 0.8294 - val_loss: 0.2503 - val_accuracy: 0.8998\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 131s 209ms/step - loss: 0.1384 - accuracy: 0.9494 - val_loss: 0.2517 - val_accuracy: 0.9081\n",
      "Validation Accuracy with W=3 and L=3: 0.9081000089645386.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 133s 212ms/step - loss: 0.3804 - accuracy: 0.8064 - val_loss: 0.2927 - val_accuracy: 0.8781\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 124s 199ms/step - loss: 0.1645 - accuracy: 0.9389 - val_loss: 0.2933 - val_accuracy: 0.8917\n",
      "Validation Accuracy with W=3 and L=4: 0.891700029373169.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 209s 334ms/step - loss: 0.3298 - accuracy: 0.8444 - val_loss: 0.2361 - val_accuracy: 0.9078\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 213s 341ms/step - loss: 0.1361 - accuracy: 0.9503 - val_loss: 0.2567 - val_accuracy: 0.9008\n",
      "Validation Accuracy with W=4 and L=2: 0.9007999897003174.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 216s 343ms/step - loss: 0.3402 - accuracy: 0.8375 - val_loss: 0.2704 - val_accuracy: 0.8922\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 222s 356ms/step - loss: 0.1437 - accuracy: 0.9474 - val_loss: 0.2603 - val_accuracy: 0.8988\n",
      "Validation Accuracy with W=4 and L=3: 0.8988000154495239.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 235s 374ms/step - loss: 0.3617 - accuracy: 0.8247 - val_loss: 0.2615 - val_accuracy: 0.8914\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 227s 363ms/step - loss: 0.1589 - accuracy: 0.9401 - val_loss: 0.2797 - val_accuracy: 0.8913\n",
      "Validation Accuracy with W=4 and L=4: 0.8913000226020813.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 297s 474ms/step - loss: 0.3235 - accuracy: 0.8476 - val_loss: 0.2369 - val_accuracy: 0.9033\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 268s 429ms/step - loss: 0.1291 - accuracy: 0.9540 - val_loss: 0.2835 - val_accuracy: 0.8999\n",
      "Validation Accuracy with W=5 and L=2: 0.8999000191688538.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 278s 443ms/step - loss: 0.3596 - accuracy: 0.8189 - val_loss: 0.2436 - val_accuracy: 0.9024\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 248s 397ms/step - loss: 0.1490 - accuracy: 0.9446 - val_loss: 0.2447 - val_accuracy: 0.9059\n",
      "Validation Accuracy with W=5 and L=3: 0.9059000015258789.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 267s 425ms/step - loss: 0.3583 - accuracy: 0.8266 - val_loss: 0.2549 - val_accuracy: 0.8962\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 265s 425ms/step - loss: 0.1572 - accuracy: 0.9416 - val_loss: 0.2835 - val_accuracy: 0.8855\n",
      "Validation Accuracy with W=5 and L=4: 0.8855000138282776.\n"
     ]
    }
   ],
   "source": [
    "# grid search for W and L\n",
    "\n",
    "W = [2, 3, 4, 5]\n",
    "L = [2, 3, 4]\n",
    "\n",
    "for w in W:\n",
    "    for l in L:\n",
    "        elaborate_model = set_elaborate_model (VOCABULARY, 200, UNIFORM_LENGTH, 30, w, l)\n",
    "        history = elaborate_model.fit (X_train, y_train, batch_size=BATCH_SIZE, epochs=2, validation_data=(X_test, y_test))\n",
    "        print (f\"Validation Accuracy with W={w} and L={l}: {history.history['val_accuracy'][-1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-designer",
   "metadata": {},
   "source": [
    "Best validation accuracy (0.9081) was obained with W=3 and L=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-belief",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
