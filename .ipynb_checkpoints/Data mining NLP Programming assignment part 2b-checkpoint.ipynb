{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Task 2-b\n",
    "In this step you will implement a “vanilla” model of the architecture. \n",
    "For this you will need to use Py- Torch or Tensorflow/Keras functional API and various layers\n",
    "like Input, Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, concatenate, Dense, etc. as well as\n",
    "other utility functions such as Tokenizer. For some of the parameters, you should consider the values\n",
    "suggested in Table 1. Please note that those values are “typical” but not necessarily optimal. First, you need to tokenize the texts and cut/pad them to a common max length size. Then you derive the train and test samples and labels. The “vanilla” model should contain the Embedding layer, a single convolution layer of only one block followed by a max-pooling layer and a single dense layer. You will report the classification accuracy of this model. (18 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, sys\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Conv1D, GlobalMaxPooling1D, MaxPooling1D, AveragePooling1D, Flatten, Concatenate, LeakyReLU, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pydot\n",
    "import graphviz\n",
    "# Collect Text \n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters (Recommendation)\n",
    "\n",
    "W or number of convolution blocks in each layer: 2-5.  \n",
    "L or number of consecuetive convolution-pooling layers: 2-4  \n",
    "Maximal length of each review sequence: 300-500    \n",
    "Dimension of word embbeddings: 150-300.  \n",
    "Train : Test split of the data samples: 4:1 or 9:1.  \n",
    "Number of filters in each convolution layer: 10-50.  \n",
    "Kernel size in each convolution block: 1-5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of convolution blocks in each layer\n",
    "# W = 1\n",
    "# Number of consecuetive convolution-pooling layers\n",
    "# L = 1\n",
    "#  Max lenght of each review sequence\n",
    "UNIFORM_LENGTH = 300\n",
    "# The dimension of the word embeddings\n",
    "WORD_EMBEDDING_DIM = 150\n",
    "# Number of filters in each convolution layer\n",
    "CONV_FILTERS = 10\n",
    "# Kernel size in each convolution block\n",
    "KERNEL_SIZE = 1\n",
    "# Vocabulary: number of most frequent words\n",
    "VOCABULARY = 30000\n",
    "# POOL_SIZE: Downsamples the input representation by taking the maximum value \n",
    "POOL_SIZE = 2\n",
    "# Training and evaluation:\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-processed data and splitting them into train and test sets. Random state is fixed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training examples: 40000\n",
      "No. of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('review_preprocessed.csv')\n",
    "training_data = df.sample(frac=0.8, random_state=25)\n",
    "testing_data = df.drop(training_data.index)\n",
    "\n",
    "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
    "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting the pandas dataframe to lists and numpy arrays\n",
    "X_train = training_data.loc[:,'review'].to_list()\n",
    "X_test = testing_data.loc[:,'review'].to_list()\n",
    "y_train = training_data.loc[:,'polarity'].to_numpy()\n",
    "y_test = testing_data.loc[:,'polarity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenize class helps us to vectorize a text corpus by tunring them into a sequence of integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = Tokenizer(num_words = VOCABULARY)\n",
    "\n",
    "t.fit_on_texts(X_train)\n",
    "X_train_enc = t.texts_to_sequences(X_train)\n",
    "X_test_enc = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we force a uniform length for each review. Longer reviews are truncated and shorted reviews and padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_enc, maxlen=UNIFORM_LENGTH)\n",
    "X_test_pad = pad_sequences(X_test_enc, maxlen=UNIFORM_LENGTH)\n",
    "\n",
    "X_train = X_train_pad\n",
    "X_test = X_test_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN vanilla model with keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fd526057a00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_existing_code(VOCABULARY, WORD_EMBEDDING_DIM, UNIFORM_LENGTH, CONV_FILTERS, KERNEL_SIZE):\n",
    "    inputs = Input(shape=(UNIFORM_LENGTH,))\n",
    "    x = Embedding(VOCABULARY, WORD_EMBEDDING_DIM, input_length=UNIFORM_LENGTH)(inputs)\n",
    "    x = Conv1D(filters=CONV_FILTERS, kernel_size=KERNEL_SIZE, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=POOL_SIZE)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense (100, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.Adam(learning_rate=lr), ### Uncomment this to tune learning rate\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    VOCABULARY = 30000\n",
    "    WORD_EMBEDDING_DIM = hp.Int(\"output_dim\", min_value=150, max_value=300, step=50)\n",
    "    UNIFORM_LENGTH = 300\n",
    "    CONV_FILTERS = hp.Int(\"filters\", min_value=10, max_value=50, step=10)\n",
    "    KERNEL_SIZE = hp.Int(\"kernel_size\", min_value=1, max_value=5, step=1)\n",
    "\n",
    "    # call existing model-building code with the hyperparameter values.\n",
    "    model = call_existing_code(\n",
    "        VOCABULARY=VOCABULARY, \n",
    "        WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, \n",
    "        UNIFORM_LENGTH=UNIFORM_LENGTH, \n",
    "        CONV_FILTERS=CONV_FILTERS, \n",
    "        KERNEL_SIZE=KERNEL_SIZE\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "build_model(kt.HyperParameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 3\n",
      "output_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 150, 'max_value': 300, 'step': 50, 'sampling': None}\n",
      "filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 50, 'step': 10, 'sampling': None}\n",
      "kernel_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 5, 'step': 1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    overwrite=True)\n",
    "    \n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 02m 28s]\n",
      "val_accuracy: 0.9053000211715698\n",
      "\n",
      "Best val_accuracy So Far: 0.9053000211715698\n",
      "Total elapsed time: 00h 10m 31s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=2, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameter for CONV_FILTERS:  30\n",
      "Optimal parameter for WORD_EMBEDDING_DIM:  200\n",
      "Optimal parameter for KERNEL_SIZE:  4\n"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "print(\"Optimal parameter for CONV_FILTERS: \", best_hps.get('filters'))\n",
    "print(\"Optimal parameter for WORD_EMBEDDING_DIM: \", best_hps.get('output_dim'))\n",
    "print(\"Optimal parameter for KERNEL_SIZE: \", best_hps.get('kernel_size'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Task 2(b)\n",
    "The optimal parameters obtained in task 2(b) were a word embedding dimension of 200 and 30 convolution filters. Those are now used in task 2(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_elaborate_model(vocabulary, word_embedding_dim, uniform_length, conv_filters, W, L):\n",
    "    inputs = Input(shape=(uniform_length,))\n",
    "    x = Embedding(vocabulary, word_embedding_dim, input_length=uniform_length)(inputs)\n",
    "    \n",
    "    #convolutional layers\n",
    "    for l in range (L):\n",
    "        c = []\n",
    "        for w in range(W):\n",
    "            if l == 0:\n",
    "                c.append(Conv1D(filters=conv_filters, kernel_size=w+1, activation='relu')(x))\n",
    "            else:\n",
    "                c.append(Conv1D(filters=conv_filters, kernel_size=w+1, activation='relu')(p[w]))\n",
    "        p = []\n",
    "        for w in range(W):    \n",
    "            p.append(MaxPooling1D(pool_size=POOL_SIZE)(c[w]))\n",
    "\n",
    "    x = Concatenate(axis=1)(p)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense (100, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    plot_model(model, to_file='elaborate_model.png', show_shapes=True)\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.Adam(learning_rate=lr), ### Uncomment this to tune learning rate\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "elaborate_model = set_elaborate_model(VOCABULARY, 200, UNIFORM_LENGTH, 30, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 138s 218ms/step - loss: 0.3266 - accuracy: 0.8465 - val_loss: 0.2356 - val_accuracy: 0.9060\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 132s 212ms/step - loss: 0.1291 - accuracy: 0.9531 - val_loss: 0.2630 - val_accuracy: 0.9018\n"
     ]
    }
   ],
   "source": [
    "history = elaborate_model.fit (X_train, y_train, batch_size=BATCH_SIZE, epochs=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8991000056266785\n"
     ]
    }
   ],
   "source": [
    "print (history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 86s 136ms/step - loss: 0.3464 - accuracy: 0.8299 - val_loss: 0.2461 - val_accuracy: 0.8991\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 86s 137ms/step - loss: 0.1359 - accuracy: 0.9508 - val_loss: 0.2648 - val_accuracy: 0.8971\n",
      "Validation Accuracy with W=2 and L=2: 0.8970999717712402.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 92s 146ms/step - loss: 0.3404 - accuracy: 0.8346 - val_loss: 0.2496 - val_accuracy: 0.8988\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 90s 143ms/step - loss: 0.1518 - accuracy: 0.9445 - val_loss: 0.2987 - val_accuracy: 0.8880\n",
      "Validation Accuracy with W=2 and L=3: 0.8880000114440918.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 93s 148ms/step - loss: 0.3680 - accuracy: 0.8261 - val_loss: 0.2775 - val_accuracy: 0.8841\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.1752 - accuracy: 0.9333 - val_loss: 0.2792 - val_accuracy: 0.8918\n",
      "Validation Accuracy with W=2 and L=4: 0.8917999863624573.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 126s 201ms/step - loss: 0.3255 - accuracy: 0.8475 - val_loss: 0.2419 - val_accuracy: 0.9052\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 126s 202ms/step - loss: 0.1266 - accuracy: 0.9543 - val_loss: 0.2702 - val_accuracy: 0.9010\n",
      "Validation Accuracy with W=3 and L=2: 0.9010000228881836.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 133s 211ms/step - loss: 0.3312 - accuracy: 0.8446 - val_loss: 0.2553 - val_accuracy: 0.8973\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 133s 213ms/step - loss: 0.1477 - accuracy: 0.9452 - val_loss: 0.2695 - val_accuracy: 0.9004\n",
      "Validation Accuracy with W=3 and L=3: 0.9003999829292297.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 139s 220ms/step - loss: 0.3526 - accuracy: 0.8295 - val_loss: 0.2609 - val_accuracy: 0.8949\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 137s 218ms/step - loss: 0.1576 - accuracy: 0.9420 - val_loss: 0.2816 - val_accuracy: 0.8907\n",
      "Validation Accuracy with W=3 and L=4: 0.8906999826431274.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 184s 292ms/step - loss: 0.3310 - accuracy: 0.8451 - val_loss: 0.2464 - val_accuracy: 0.8998\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 184s 294ms/step - loss: 0.1345 - accuracy: 0.9510 - val_loss: 0.2686 - val_accuracy: 0.8989\n",
      "Validation Accuracy with W=4 and L=2: 0.8988999724388123.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 201s 320ms/step - loss: 0.3321 - accuracy: 0.8430 - val_loss: 0.2338 - val_accuracy: 0.9098\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 209s 334ms/step - loss: 0.1323 - accuracy: 0.9517 - val_loss: 0.2436 - val_accuracy: 0.9053\n",
      "Validation Accuracy with W=4 and L=3: 0.9053000211715698.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 214s 339ms/step - loss: 0.3485 - accuracy: 0.8281 - val_loss: 0.2828 - val_accuracy: 0.8867\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 217s 347ms/step - loss: 0.1445 - accuracy: 0.9479 - val_loss: 0.2693 - val_accuracy: 0.9003\n",
      "Validation Accuracy with W=4 and L=4: 0.9003000259399414.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 271s 432ms/step - loss: 0.3385 - accuracy: 0.8330 - val_loss: 0.2358 - val_accuracy: 0.9068\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 263s 420ms/step - loss: 0.1344 - accuracy: 0.9516 - val_loss: 0.2535 - val_accuracy: 0.9059\n",
      "Validation Accuracy with W=5 and L=2: 0.9059000015258789.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 275s 435ms/step - loss: 0.3363 - accuracy: 0.8399 - val_loss: 0.2424 - val_accuracy: 0.9026\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 271s 434ms/step - loss: 0.1478 - accuracy: 0.9448 - val_loss: 0.2538 - val_accuracy: 0.9011\n",
      "Validation Accuracy with W=5 and L=3: 0.9010999798774719.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 279s 443ms/step - loss: 0.3728 - accuracy: 0.8105 - val_loss: 0.2480 - val_accuracy: 0.8995\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 279s 446ms/step - loss: 0.1553 - accuracy: 0.9418 - val_loss: 0.2792 - val_accuracy: 0.8973\n",
      "Validation Accuracy with W=5 and L=4: 0.8973000049591064.\n"
     ]
    }
   ],
   "source": [
    "# grid search for W and L\n",
    "\n",
    "W = [2, 3, 4, 5]\n",
    "L = [2, 3, 4]\n",
    "\n",
    "for w in W:\n",
    "    for l in L:\n",
    "            elaborate_model = set_elaborate_model(VOCABULARY, 200, UNIFORM_LENGTH, 30, w, l)\n",
    "            history = elaborate_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=2, validation_data=(X_test, y_test))\n",
    "            print (f\"Validation Accuracy with W={w} and L={l}: {history.history['val_accuracy'][-1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Validation Accuracy with W=5 and L=2: 0.9059000015258789."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 77s 122ms/step - loss: 0.3398 - accuracy: 0.8383 - val_loss: 0.2467 - val_accuracy: 0.8996\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 81s 129ms/step - loss: 0.1379 - accuracy: 0.9498 - val_loss: 0.2623 - val_accuracy: 0.8971\n",
      "Validation Accuracy with W=2 and L=2 and Conv_Layer=20: 0.8970999717712402.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 86s 136ms/step - loss: 0.3316 - accuracy: 0.8451 - val_loss: 0.2533 - val_accuracy: 0.8962\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 83s 133ms/step - loss: 0.1341 - accuracy: 0.9523 - val_loss: 0.2766 - val_accuracy: 0.8982\n",
      "Validation Accuracy with W=2 and L=2 and Conv_Layer=30: 0.8981999754905701.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 97s 155ms/step - loss: 0.3373 - accuracy: 0.8393 - val_loss: 0.2496 - val_accuracy: 0.9000\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 87s 139ms/step - loss: 0.1345 - accuracy: 0.9518 - val_loss: 0.2697 - val_accuracy: 0.8975\n",
      "Validation Accuracy with W=2 and L=2 and Conv_Layer=40: 0.8974999785423279.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 79s 122ms/step - loss: 0.3388 - accuracy: 0.8384 - val_loss: 0.2492 - val_accuracy: 0.9007\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 78s 125ms/step - loss: 0.1467 - accuracy: 0.9465 - val_loss: 0.2720 - val_accuracy: 0.8929\n",
      "Validation Accuracy with W=2 and L=3 and Conv_Layer=20: 0.8928999900817871.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 89s 141ms/step - loss: 0.3380 - accuracy: 0.8370 - val_loss: 0.2431 - val_accuracy: 0.9013\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 90s 143ms/step - loss: 0.1409 - accuracy: 0.9480 - val_loss: 0.2640 - val_accuracy: 0.8999\n",
      "Validation Accuracy with W=2 and L=3 and Conv_Layer=30: 0.8999000191688538.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 99s 158ms/step - loss: 0.3408 - accuracy: 0.8345 - val_loss: 0.2625 - val_accuracy: 0.8962\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 102s 163ms/step - loss: 0.1384 - accuracy: 0.9496 - val_loss: 0.2595 - val_accuracy: 0.9009\n",
      "Validation Accuracy with W=2 and L=3 and Conv_Layer=40: 0.9009000062942505.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 90s 143ms/step - loss: 0.3514 - accuracy: 0.8353 - val_loss: 0.2722 - val_accuracy: 0.8883\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 90s 144ms/step - loss: 0.1548 - accuracy: 0.9428 - val_loss: 0.3475 - val_accuracy: 0.8777\n",
      "Validation Accuracy with W=2 and L=4 and Conv_Layer=20: 0.8776999711990356.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 101s 160ms/step - loss: 0.3574 - accuracy: 0.8262 - val_loss: 0.2721 - val_accuracy: 0.8903\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 94s 150ms/step - loss: 0.1539 - accuracy: 0.9427 - val_loss: 0.2740 - val_accuracy: 0.8926\n",
      "Validation Accuracy with W=2 and L=4 and Conv_Layer=30: 0.8925999999046326.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 103s 162ms/step - loss: 0.3562 - accuracy: 0.8297 - val_loss: 0.3057 - val_accuracy: 0.8731\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 102s 163ms/step - loss: 0.1509 - accuracy: 0.9431 - val_loss: 0.2732 - val_accuracy: 0.8973\n",
      "Validation Accuracy with W=2 and L=4 and Conv_Layer=40: 0.8973000049591064.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 119s 190ms/step - loss: 0.3302 - accuracy: 0.8448 - val_loss: 0.2414 - val_accuracy: 0.9035\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 118s 190ms/step - loss: 0.1256 - accuracy: 0.9564 - val_loss: 0.2642 - val_accuracy: 0.8989\n",
      "Validation Accuracy with W=3 and L=2 and Conv_Layer=20: 0.8988999724388123.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 130s 207ms/step - loss: 0.3280 - accuracy: 0.8470 - val_loss: 0.2489 - val_accuracy: 0.8985\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 129s 207ms/step - loss: 0.1339 - accuracy: 0.9518 - val_loss: 0.3386 - val_accuracy: 0.8818\n",
      "Validation Accuracy with W=3 and L=2 and Conv_Layer=30: 0.8817999958992004.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 144s 229ms/step - loss: 0.3156 - accuracy: 0.8561 - val_loss: 0.2569 - val_accuracy: 0.8968\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 143s 229ms/step - loss: 0.1247 - accuracy: 0.9548 - val_loss: 0.2650 - val_accuracy: 0.8983\n",
      "Validation Accuracy with W=3 and L=2 and Conv_Layer=40: 0.8982999920845032.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 126s 200ms/step - loss: 0.3340 - accuracy: 0.8399 - val_loss: 0.2643 - val_accuracy: 0.8935\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 126s 201ms/step - loss: 0.1358 - accuracy: 0.9517 - val_loss: 0.2548 - val_accuracy: 0.9059\n",
      "Validation Accuracy with W=3 and L=3 and Conv_Layer=20: 0.9059000015258789.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 139s 220ms/step - loss: 0.3453 - accuracy: 0.8313 - val_loss: 0.2539 - val_accuracy: 0.8965\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 135s 215ms/step - loss: 0.1464 - accuracy: 0.9457 - val_loss: 0.2679 - val_accuracy: 0.8987\n",
      "Validation Accuracy with W=3 and L=3 and Conv_Layer=30: 0.8986999988555908.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 150s 239ms/step - loss: 0.3634 - accuracy: 0.8146 - val_loss: 0.2623 - val_accuracy: 0.8933\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 168s 269ms/step - loss: 0.1679 - accuracy: 0.9362 - val_loss: 0.2847 - val_accuracy: 0.8886\n",
      "Validation Accuracy with W=3 and L=3 and Conv_Layer=40: 0.8885999917984009.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 136s 216ms/step - loss: 0.3588 - accuracy: 0.8228 - val_loss: 0.2502 - val_accuracy: 0.8994\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 129s 207ms/step - loss: 0.1441 - accuracy: 0.9466 - val_loss: 0.2782 - val_accuracy: 0.8969\n",
      "Validation Accuracy with W=3 and L=4 and Conv_Layer=20: 0.8968999981880188.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 146s 232ms/step - loss: 0.3526 - accuracy: 0.8341 - val_loss: 0.2796 - val_accuracy: 0.8889\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 154s 246ms/step - loss: 0.1564 - accuracy: 0.9427 - val_loss: 0.2818 - val_accuracy: 0.8904\n",
      "Validation Accuracy with W=3 and L=4 and Conv_Layer=30: 0.8903999924659729.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 180s 286ms/step - loss: 0.3612 - accuracy: 0.8270 - val_loss: 0.2609 - val_accuracy: 0.8942\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 176s 282ms/step - loss: 0.1576 - accuracy: 0.9412 - val_loss: 0.3365 - val_accuracy: 0.8783\n",
      "Validation Accuracy with W=3 and L=4 and Conv_Layer=40: 0.8783000111579895.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 204s 325ms/step - loss: 0.3373 - accuracy: 0.8383 - val_loss: 0.2340 - val_accuracy: 0.9078\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 181s 290ms/step - loss: 0.1321 - accuracy: 0.9527 - val_loss: 0.2611 - val_accuracy: 0.9027\n",
      "Validation Accuracy with W=4 and L=2 and Conv_Layer=20: 0.9027000069618225.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 217s 346ms/step - loss: 0.3456 - accuracy: 0.8334 - val_loss: 0.2408 - val_accuracy: 0.9027\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 191s 305ms/step - loss: 0.1439 - accuracy: 0.9476 - val_loss: 0.2988 - val_accuracy: 0.8900\n",
      "Validation Accuracy with W=4 and L=2 and Conv_Layer=30: 0.8899999856948853.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 223s 355ms/step - loss: 0.3229 - accuracy: 0.8483 - val_loss: 0.2354 - val_accuracy: 0.9054\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 216s 346ms/step - loss: 0.1305 - accuracy: 0.9520 - val_loss: 0.2951 - val_accuracy: 0.8918\n",
      "Validation Accuracy with W=4 and L=2 and Conv_Layer=40: 0.8917999863624573.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 188s 298ms/step - loss: 0.3458 - accuracy: 0.8284 - val_loss: 0.2348 - val_accuracy: 0.9074\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 184s 295ms/step - loss: 0.1386 - accuracy: 0.9503 - val_loss: 0.2592 - val_accuracy: 0.9015\n",
      "Validation Accuracy with W=4 and L=3 and Conv_Layer=20: 0.9014999866485596.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 219s 346ms/step - loss: 0.3439 - accuracy: 0.8316 - val_loss: 0.2682 - val_accuracy: 0.8942\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 215s 344ms/step - loss: 0.1388 - accuracy: 0.9493 - val_loss: 0.2528 - val_accuracy: 0.9021\n",
      "Validation Accuracy with W=4 and L=3 and Conv_Layer=30: 0.9021000266075134.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 217s 346ms/step - loss: 0.3348 - accuracy: 0.8396 - val_loss: 0.2715 - val_accuracy: 0.8885\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 216s 346ms/step - loss: 0.1471 - accuracy: 0.9465 - val_loss: 0.2715 - val_accuracy: 0.8986\n",
      "Validation Accuracy with W=4 and L=3 and Conv_Layer=40: 0.8985999822616577.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 177s 281ms/step - loss: 0.3598 - accuracy: 0.8249 - val_loss: 0.2772 - val_accuracy: 0.8850\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 175s 281ms/step - loss: 0.1530 - accuracy: 0.9425 - val_loss: 0.2681 - val_accuracy: 0.8953\n",
      "Validation Accuracy with W=4 and L=4 and Conv_Layer=20: 0.8952999711036682.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 200s 317ms/step - loss: 0.3607 - accuracy: 0.8238 - val_loss: 0.2587 - val_accuracy: 0.8955\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 197s 316ms/step - loss: 0.1518 - accuracy: 0.9444 - val_loss: 0.2595 - val_accuracy: 0.8944\n",
      "Validation Accuracy with W=4 and L=4 and Conv_Layer=30: 0.8944000005722046.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 220s 349ms/step - loss: 0.3636 - accuracy: 0.8220 - val_loss: 0.2610 - val_accuracy: 0.8933\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 229s 366ms/step - loss: 0.1608 - accuracy: 0.9391 - val_loss: 0.2853 - val_accuracy: 0.8919\n",
      "Validation Accuracy with W=4 and L=4 and Conv_Layer=40: 0.8919000029563904.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 246s 390ms/step - loss: 0.3295 - accuracy: 0.8431 - val_loss: 0.2387 - val_accuracy: 0.9032\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 226s 361ms/step - loss: 0.1263 - accuracy: 0.9556 - val_loss: 0.2537 - val_accuracy: 0.9046\n",
      "Validation Accuracy with W=5 and L=2 and Conv_Layer=20: 0.9046000242233276.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 243s 386ms/step - loss: 0.3203 - accuracy: 0.8530 - val_loss: 0.2428 - val_accuracy: 0.9006\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 235s 376ms/step - loss: 0.1301 - accuracy: 0.9526 - val_loss: 0.2564 - val_accuracy: 0.9078\n",
      "Validation Accuracy with W=5 and L=2 and Conv_Layer=30: 0.907800018787384.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 263s 419ms/step - loss: 0.3208 - accuracy: 0.8494 - val_loss: 0.2287 - val_accuracy: 0.9095\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 262s 419ms/step - loss: 0.1290 - accuracy: 0.9541 - val_loss: 0.2663 - val_accuracy: 0.8912\n",
      "Validation Accuracy with W=5 and L=2 and Conv_Layer=40: 0.8912000060081482.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 217s 344ms/step - loss: 0.3405 - accuracy: 0.8347 - val_loss: 0.2337 - val_accuracy: 0.9071\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 215s 343ms/step - loss: 0.1370 - accuracy: 0.9504 - val_loss: 0.2510 - val_accuracy: 0.9047\n",
      "Validation Accuracy with W=5 and L=3 and Conv_Layer=20: 0.904699981212616.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 250s 398ms/step - loss: 0.3305 - accuracy: 0.8425 - val_loss: 0.2370 - val_accuracy: 0.9049\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 245s 392ms/step - loss: 0.1425 - accuracy: 0.9482 - val_loss: 0.2568 - val_accuracy: 0.9042\n",
      "Validation Accuracy with W=5 and L=3 and Conv_Layer=30: 0.90420001745224.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 274s 435ms/step - loss: 0.4133 - accuracy: 0.7735 - val_loss: 0.2551 - val_accuracy: 0.9002\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 271s 433ms/step - loss: 0.1677 - accuracy: 0.9377 - val_loss: 0.2709 - val_accuracy: 0.8908\n",
      "Validation Accuracy with W=5 and L=3 and Conv_Layer=40: 0.8907999992370605.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 221s 350ms/step - loss: 0.3436 - accuracy: 0.8375 - val_loss: 0.2503 - val_accuracy: 0.8992\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 218s 348ms/step - loss: 0.1417 - accuracy: 0.9486 - val_loss: 0.2762 - val_accuracy: 0.8947\n",
      "Validation Accuracy with W=5 and L=4 and Conv_Layer=20: 0.8946999907493591.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 242s 385ms/step - loss: 0.3581 - accuracy: 0.8250 - val_loss: 0.2839 - val_accuracy: 0.8870\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 240s 383ms/step - loss: 0.1617 - accuracy: 0.9392 - val_loss: 0.2851 - val_accuracy: 0.8937\n",
      "Validation Accuracy with W=5 and L=4 and Conv_Layer=30: 0.8937000036239624.\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 269s 428ms/step - loss: 0.3709 - accuracy: 0.8120 - val_loss: 0.2525 - val_accuracy: 0.8980\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 285s 456ms/step - loss: 0.1534 - accuracy: 0.9434 - val_loss: 0.2605 - val_accuracy: 0.8988\n",
      "Validation Accuracy with W=5 and L=4 and Conv_Layer=40: 0.8988000154495239.\n"
     ]
    }
   ],
   "source": [
    "# grid search for W and L and number of filters in convolution layer\n",
    "\n",
    "W = [2, 3, 4, 5]\n",
    "L = [2, 3, 4]\n",
    "Conv_Layer = [20, 30, 40]\n",
    "\n",
    "for w in W:\n",
    "    for l in L:\n",
    "        for c in Conv_Layer:\n",
    "            elaborate_model = set_elaborate_model(VOCABULARY, 200, UNIFORM_LENGTH, c, w, l)\n",
    "            history = elaborate_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=2, validation_data=(X_test, y_test))\n",
    "            print (f\"Validation Accuracy with W={w} and L={l} and Conv_Layer={c}: {history.history['val_accuracy'][-1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Validation Accuracy with W=5 and L=2 and Conv_Layer=30: 0.907800018787384."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pooling\n",
    "def set_elaborate_model(vocabulary, word_embedding_dim, uniform_length, conv_filters, W, L):\n",
    "    inputs = Input(shape=(uniform_length,))\n",
    "    x = Embedding(vocabulary, word_embedding_dim, input_length=uniform_length)(inputs)\n",
    "    \n",
    "    #convolutional layers\n",
    "    for l in range (L):\n",
    "        c = []\n",
    "        for w in range(W):\n",
    "            if l == 0:\n",
    "                c.append(Conv1D(filters=conv_filters, kernel_size=w+1, activation='relu')(x))\n",
    "            else:\n",
    "                c.append(Conv1D(filters=conv_filters, kernel_size=w+1, activation='relu')(p[w]))\n",
    "        p = []\n",
    "        for w in range(W):    \n",
    "            p.append(AveragePooling1D(pool_size=POOL_SIZE)(c[w]))\n",
    "\n",
    "    x = Concatenate(axis=1)(p)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense (100, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    plot_model(model, to_file='elaborate_model_average.png', show_shapes=True)\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.Adam(learning_rate=lr), ### Uncomment this to tune learning rate\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "elaborate_model = set_elaborate_model (VOCABULARY, 200, UNIFORM_LENGTH, 30, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 139s 221ms/step - loss: 0.3294 - accuracy: 0.8439 - val_loss: 0.2415 - val_accuracy: 0.9030\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 131s 210ms/step - loss: 0.1450 - accuracy: 0.9467 - val_loss: 0.2563 - val_accuracy: 0.9005\n"
     ]
    }
   ],
   "source": [
    "history = elaborate_model.fit (X_train, y_train, batch_size=BATCH_SIZE, epochs=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a lot of regularization\n",
    "\n",
    "def set_elaborate_model(vocabulary, word_embedding_dim, uniform_length, conv_filters, W, L):\n",
    "    inputs = Input(shape=(uniform_length,))\n",
    "    x = Embedding(vocabulary, word_embedding_dim, input_length=uniform_length)(inputs)\n",
    "    x = Dropout (0.4)(x)\n",
    "\n",
    "    #convolutional layers\n",
    "    for l in range (L):\n",
    "        c = []\n",
    "        for w in range(W):\n",
    "            if l == 0:\n",
    "                c.append (Conv1D(filters=conv_filters, kernel_size=w+3, activation=LeakyReLU(alpha=0.1),\n",
    "                          kernel_regularizer=l2(1e-4),\n",
    "                          bias_regularizer=l2(1e-4),\n",
    "                          activity_regularizer=l2(1e-4))(x))\n",
    "            else:\n",
    "                c.append (Conv1D(filters=conv_filters, kernel_size=w+3, activation=LeakyReLU(alpha=0.1),\n",
    "                          kernel_regularizer=l2(1e-4),\n",
    "                          bias_regularizer=l2(1e-4),\n",
    "                          activity_regularizer=l2(1e-4))(p[w-1]))\n",
    "        p = []\n",
    "        for w in range(W):    \n",
    "            p.append (MaxPooling1D(pool_size=POOL_SIZE)(c[w]))\n",
    "\n",
    "    x = Concatenate(axis=1)(p)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense (1000, activation=LeakyReLU(alpha=0.1), activity_regularizer=l2(1e-2))(x)\n",
    "    x = Dropout (0.4)(x)\n",
    "    x = Dense (100, activation=LeakyReLU(alpha=0.1), activity_regularizer=l2(1e-2))(x)\n",
    "    x = Dropout (0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    plot_model(model, to_file='elaborate_model.png', show_shapes=True)\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.Adam(learning_rate=lr), ### Uncomment this to tune learning rate\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 109s 173ms/step - loss: 0.3970 - accuracy: 0.8299 - val_loss: 0.3046 - val_accuracy: 0.8943\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 109s 174ms/step - loss: 0.2023 - accuracy: 0.9424 - val_loss: 0.3129 - val_accuracy: 0.8958\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 100s 160ms/step - loss: 0.1288 - accuracy: 0.9703 - val_loss: 0.3943 - val_accuracy: 0.8781\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 97s 155ms/step - loss: 0.1025 - accuracy: 0.9796 - val_loss: 0.3456 - val_accuracy: 0.8922\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 97s 155ms/step - loss: 0.0899 - accuracy: 0.9834 - val_loss: 0.4641 - val_accuracy: 0.8762\n"
     ]
    }
   ],
   "source": [
    "elaborate_model = set_elaborate_model (VOCABULARY, 200, UNIFORM_LENGTH, 30, 2, 2)\n",
    "history = elaborate_model.fit (X_train, y_train, batch_size=BATCH_SIZE, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
