{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task 2-b\n",
    "In this step you will implement a “vanilla” model of the architecture. \n",
    "For this you will need to use Py- Torch or Tensorflow/Keras functional API and various layers\n",
    "like Input, Embedding, Conv1D, Dropout, MaxPooling1D, Flatten, concatenate, Dense, etc. as well as\n",
    "other utility functions such as Tokenizer. For some of the parameters, you should consider the values\n",
    "suggested in Table 1. Please note that those values are “typical” but not necessarily optimal. First, you need to tokenize the texts and cut/pad them to a common max length size. Then you derive the train and test samples and labels. The “vanilla” model should contain the Embedding layer, a single convolution layer of only one block followed by a max-pooling layer and a single dense layer. You will report the classification accuracy of this model. (18 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from -r requirements.txt (line 1)) (1.17.4)\n",
      "Requirement already satisfied: pandas in /home/robert/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.1.5)\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting keras_tuner\n",
      "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 3.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting graphviz\n",
      "  Downloading graphviz-0.19.1-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas->-r requirements.txt (line 2)) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas->-r requirements.txt (line 2)) (2019.3)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/lib/python3/dist-packages (from pydot->-r requirements.txt (line 3)) (2.4.6)\n",
      "Requirement already satisfied: tensorboard in /home/robert/.local/lib/python3.8/site-packages (from keras_tuner->-r requirements.txt (line 4)) (2.7.0)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from keras_tuner->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: packaging in /home/robert/.local/lib/python3.8/site-packages (from keras_tuner->-r requirements.txt (line 4)) (20.8)\n",
      "Requirement already satisfied: ipython in /usr/lib/python3/dist-packages (from keras_tuner->-r requirements.txt (line 4)) (7.13.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from keras_tuner->-r requirements.txt (line 4)) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (1.42.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (3.19.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (0.34.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (45.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/robert/.local/lib/python3.8/site-packages (from tensorboard->keras_tuner->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: pexpect in /usr/lib/python3/dist-packages (from ipython->keras_tuner->-r requirements.txt (line 4)) (4.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/robert/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio>=1.24.3->tensorboard->keras_tuner->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/robert/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->keras_tuner->-r requirements.txt (line 4)) (4.8.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/robert/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner->-r requirements.txt (line 4)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/robert/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/robert/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard->keras_tuner->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/robert/.local/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard->keras_tuner->-r requirements.txt (line 4)) (0.4.8)\n",
      "Installing collected packages: pydot, kt-legacy, keras-tuner, graphviz\n",
      "Successfully installed graphviz-0.19.1 keras-tuner-1.1.0 kt-legacy-1.0.4 pydot-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random, sys\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pydot\n",
    "import graphviz\n",
    "# Collect Text \n",
    "import pandas as pd\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters (Recommendation)\n",
    "\n",
    "W or number of convolution blocks in each layer: 2-5.  \n",
    "L or number of consecuetive convolution-pooling layers: 2-4  \n",
    "Maximal length of each review sequence: 300-500    \n",
    "Dimension of word embbeddings: 150-300.  \n",
    "Train : Test split of the data samples: 4:1 or 9:1.  \n",
    "Number of filters in each convolution layer: 10-50.  \n",
    "Kernel size in each convolution block: 1-5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of convolution blocks in each layer\n",
    "# W = 1\n",
    "# Number of consecuetive convolution-pooling layers\n",
    "# L = 1\n",
    "#  Max lenght of each review sequence\n",
    "UNIFORM_LENGTH = 600\n",
    "# The dimension of the word embeddings\n",
    "WORD_EMBEDDING_DIM = 150\n",
    "# Number of filters in each convolution layer\n",
    "CONV_FILTERS = 10\n",
    "# Kernel size in each convolution block\n",
    "KERNEL_SIZE = 1\n",
    "# Vocabulary: number of most frequent words\n",
    "VOCABULARY = 30000\n",
    "# POOL_SIZE: Downsamples the input representation by taking the maximum value \n",
    "POOL_SIZE = 2\n",
    "# Training and evaluation:\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pre-processed data and splitting them into train and test sets. Random state is fixed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training examples: 40000\n",
      "No. of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('review_preprocessed.csv')\n",
    "training_data = df.sample(frac=0.8, random_state=25)\n",
    "testing_data = df.drop(training_data.index)\n",
    "\n",
    "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
    "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting the pandas dataframe to lists and numpy arrays\n",
    "X_train = training_data.loc[:,'review'].to_list()\n",
    "X_test = testing_data.loc[:,'review'].to_list()\n",
    "y_train = training_data.loc[:,'polarity'].to_numpy()\n",
    "y_test = testing_data.loc[:,'polarity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenize class helps us to vectorize a text corpus by tunring them into a sequence of integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = Tokenizer(num_words = VOCABULARY)\n",
    "\n",
    "t.fit_on_texts(X_train)\n",
    "X_train_enc = t.texts_to_sequences(X_train)\n",
    "X_test_enc = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we force a uniform length for each review. Longer reviews are truncated and shorted reviews and padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_enc, maxlen=UNIFORM_LENGTH)\n",
    "X_test_pad = pad_sequences(X_test_enc, maxlen=UNIFORM_LENGTH)\n",
    "\n",
    "X_train = X_train_pad\n",
    "X_test = X_test_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model with keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x158512b50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_existing_code(VOCABULARY, WORD_EMBEDDING_DIM, UNIFORM_LENGTH, CONV_FILTERS, KERNEL_SIZE):\n",
    "    inputs = Input(shape=(UNIFORM_LENGTH,))\n",
    "    x = Embedding(VOCABULARY, WORD_EMBEDDING_DIM, input_length=UNIFORM_LENGTH)(inputs)\n",
    "    x = Conv1D(filters=CONV_FILTERS, kernel_size=KERNEL_SIZE, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=POOL_SIZE)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    model.compile(\n",
    "        #optimizer=keras.optimizers.Adam(learning_rate=lr), ### Uncomment this to tune learning rate\n",
    "        optimizer='adam',\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    VOCABULARY = 30000\n",
    "    WORD_EMBEDDING_DIM = hp.Int(\"output_dim\", min_value=150, max_value=300, step=50)\n",
    "    UNIFORM_LENGTH = 600\n",
    "    CONV_FILTERS = hp.Int(\"filters\", min_value=10, max_value=50, step=10)\n",
    "    KERNEL_SIZE = hp.Int(\"kernel_size\", min_value=1, max_value=5, step=1)\n",
    "\n",
    "    # call existing model-building code with the hyperparameter values.\n",
    "    model = call_existing_code(\n",
    "        VOCABULARY=VOCABULARY, \n",
    "        WORD_EMBEDDING_DIM=WORD_EMBEDDING_DIM, \n",
    "        UNIFORM_LENGTH=UNIFORM_LENGTH, \n",
    "        CONV_FILTERS=CONV_FILTERS, \n",
    "        KERNEL_SIZE=KERNEL_SIZE\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "build_model(kt.HyperParameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 3\n",
      "output_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 150, 'max_value': 300, 'step': 50, 'sampling': None}\n",
      "filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 50, 'step': 10, 'sampling': None}\n",
      "kernel_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 5, 'step': 1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=5,\n",
    "    overwrite=True)\n",
    "    \n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 47s]\n",
      "accuracy: 0.5533413887023926\n",
      "\n",
      "Best accuracy So Far: 0.5559917688369751\n",
      "Total elapsed time: 00h 05m 06s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=2, batch_size=BATCH_SIZE, validation_data=(X_test, y_test))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameter for CONV_FILTERS:  20\n",
      "Optimal parameter for WORD_EMBEDDING_DIM:  150\n",
      "Optimal parameter for KERNEL_SIZE:  5\n"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "print(\"Optimal parameter for CONV_FILTERS: \", best_hps.get('filters'))\n",
    "print(\"Optimal parameter for WORD_EMBEDDING_DIM: \", best_hps.get('output_dim'))\n",
    "print(\"Optimal parameter for KERNEL_SIZE: \", best_hps.get('kernel_size'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
